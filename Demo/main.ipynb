{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math\n",
    "import nltk.data\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer # embedding câu\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim = 384, hidden_dim = 128, output_dim = 1, dropout = 0.2, numlayers = 1, bidirectional = False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_layers = numlayers\n",
    "        self.D = 2 if bidirectional else 1\n",
    "\n",
    "        self.hidden_dim = hidden_dim # có giá trị tự do\n",
    "        self.embedding_dim = embedding_dim # chiều của embedding, vd: [1,2,3,...300]: 1 embedding có kích thước là 300\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,batch_first=True, num_layers = numlayers, dropout = dropout, bidirectional=bidirectional) # đầu vào của LSTM là có kích thước embedding và đầu ra có kích thước hidden, xây dựng 1 mô hình LSTM\n",
    "        # input_size = embedding_dim; hidden_size, num_layer\n",
    "        # tuning ở ngay trên\n",
    "        self.fc = nn.Linear(self.D*hidden_dim, output_dim) # 1 fully conected để làm đầu ra\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        '''\n",
    "        đầu vào input lần lượt là: batch_size, sequence_length, embedding_dim\n",
    "        '''\n",
    "        # inputs = torch.nn.utils.rnn.pack_padded_sequence(inputs, )\n",
    "        batch_size = inputs.batch_sizes[0].item()#input ở đây chính là 1 batch mà chúng ta cho vào, và tập huấn luyện của chúng ta chứa những inputs này\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_out, _ = self.lstm(inputs, hidden)\n",
    "\n",
    "        # Giải nén lstm_out\n",
    "        padded_outputs, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "\n",
    "        padded_outputs = padded_outputs[:, -1, :]\n",
    "\n",
    "        output = self.fc(padded_outputs)\n",
    "        return output\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.D * self.num_layers, batch_size, self.hidden_dim),# gồm 1 phần chứa batch_size 'phần', 'phần' chứa hidden_dim units\n",
    "                torch.zeros(self.D * self.num_layers, batch_size, self.hidden_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2embedding(model_embedding, sentence):\n",
    "    model = model_embedding\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embedding = model.encode(sentence)\n",
    "\n",
    "    return torch.Tensor(embedding) if len(embedding.shape) >= 2 else torch.Tensor([embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "def predict(model,model_embedding,text, speed = 1):\n",
    "    tokenizer = nltk.data.load('english.pickle')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    max_sequence_length = math.ceil(len(tokens) / speed)\n",
    "\n",
    "    indices = random.sample(range(len(tokens)), max_sequence_length)\n",
    "    indices.sort()\n",
    "    tokens = [tokens[i] for i in indices]\n",
    "\n",
    "    new_data = text2embedding(model_embedding,tokens)\n",
    "    new_data = new_data.unsqueeze(0)\n",
    "\n",
    "    sequence_lengths = [len(seq) for seq in new_data]\n",
    "\n",
    "    new_data = pack_padded_sequence(new_data, sequence_lengths, batch_first=True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = new_data\n",
    "        outputs = model(inputs)#gọi hàm forward\n",
    "\n",
    "        proba_label = torch.sigmoid(outputs)\n",
    "        predict_label = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "\n",
    "    return proba_label.item(), 'positive' if predict_label == 1 else 'negative'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\miniconda3\\envs\\manhtms1\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_best_parameter_model = 'best_params_model.pkl'\n",
    "model = LSTMModel()\n",
    "model.load_state_dict(torch.load(path_best_parameter_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "As someone who relies on multiple electronic devices daily, I was in need of a reliable and affordable power strip. The AmazonBasics 6-Outlet, 200 Joule Surge Protector Power Strip has proven to be a great solution, providing functionality and peace of mind at an attractive price point.\n",
    "\n",
    "The AmazonBasics power strip offers six outlets, which is more than enough to accommodate my various devices, including my computer, monitor, speakers, and phone charger. The 2-foot cord is not the longest, but it has been sufficient for my needs and helps to minimize cable clutter.\n",
    "\n",
    "The 200 Joule surge protection rating provides a basic level of protection for my devices, safeguarding them against power surges and spikes. While it may not be the highest level of protection available, it is suitable for everyday use and offers reassurance that my valuable electronics are secure.\n",
    "\n",
    "The power strip's design is simple and unobtrusive, making it easy to blend into any room or workspace. The white color and slim profile do not draw attention, allowing it to integrate seamlessly into my setup.\n",
    "\n",
    "One minor drawback is the lack of USB ports for charging devices directly, but this is not a deal-breaker considering the budget-friendly price and the primary purpose of the power strip.\n",
    "\n",
    "Overall, the AmazonBasics 6-Outlet, 200 Joule Surge Protector Power Strip is an excellent value, offering reliable functionality and basic surge protection at an affordable price. For those seeking a simple and effective solution for managing multiple devices, this power strip is a solid choice.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9763833284378052, 'positive')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, model_embedding, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manhtms1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
